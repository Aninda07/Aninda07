{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_1_Final_Project(YOLOv5).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "922ff578fbd14f778461a6409eb67456": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d0aa424944624b8cb8d9c03459d189dd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9ee8c91d23154526bef1183e878fc186",
              "IPY_MODEL_3100eb5035614d64b4e3c8421557483a",
              "IPY_MODEL_0e8d925cb68742eba1a8346fbf6bd051"
            ]
          }
        },
        "d0aa424944624b8cb8d9c03459d189dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9ee8c91d23154526bef1183e878fc186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e433cb5f55f741679bee86816836d4d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_673d7c6ef05749d1876c18b10ce5bc79"
          }
        },
        "3100eb5035614d64b4e3c8421557483a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0bb77fc0b37c45ef8d58ddb116843feb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 6984509,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 6984509,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_667c591204eb4042bc3db412a90f7428"
          }
        },
        "0e8d925cb68742eba1a8346fbf6bd051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6a147476fe0b4baf8b1bc1619b342193",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 6.66M/6.66M [00:00&lt;00:00, 35.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d9fb5e938ef40d6bc5b87c9db8f7b31"
          }
        },
        "e433cb5f55f741679bee86816836d4d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "673d7c6ef05749d1876c18b10ce5bc79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bb77fc0b37c45ef8d58ddb116843feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "667c591204eb4042bc3db412a90f7428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a147476fe0b4baf8b1bc1619b342193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d9fb5e938ef40d6bc5b87c9db8f7b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aninda07/CVPR/blob/main/Group_1_Final_Project(YOLOv5).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dQYs56yns5n"
      },
      "source": [
        "import argparse\n",
        "\n",
        "import torch.distributed as dist\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import torch.utils.data\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import test  # import test.py to get mAP after each epoch\n",
        "from models.yolo import Model\n",
        "from utils import google_utils\n",
        "from utils.datasets import *\n",
        "from utils.utils import *\n",
        "\n",
        "# Hyperparameters\n",
        "hyp = {'optimizer': 'SGD',  # ['adam', 'SGD', None] if none, default is SGD\n",
        "       'lr0': 0.01,  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
        "       'momentum': 0.937,  # SGD momentum/Adam beta1\n",
        "       'weight_decay': 5e-4,  # optimizer weight decay\n",
        "       'giou': 0.05,  # giou loss gain\n",
        "       'cls': 0.58,  # cls loss gain\n",
        "       'cls_pw': 1.0,  # cls BCELoss positive_weight\n",
        "       'obj': 1.0,  # obj loss gain (*=img_size/320 if img_size != 320)\n",
        "       'obj_pw': 1.0,  # obj BCELoss positive_weight\n",
        "       'iou_t': 0.20,  # iou training threshold\n",
        "       'anchor_t': 4.0,  # anchor-multiple threshold\n",
        "       'fl_gamma': 0.0,  # focal loss gamma (efficientDet default is gamma=1.5)\n",
        "       'hsv_h': 0.014,  # image HSV-Hue augmentation (fraction)\n",
        "       'hsv_s': 0.68,  # image HSV-Saturation augmentation (fraction)\n",
        "       'hsv_v': 0.36,  # image HSV-Value augmentation (fraction)\n",
        "       'degrees': 0.0,  # image rotation (+/- deg)\n",
        "       'translate': 0.0,  # image translation (+/- fraction)\n",
        "       'scale': 0.5,  # image scale (+/- gain)\n",
        "       'shear': 0.0}  # image shear (+/- deg)\n",
        "\n",
        "\n",
        "def train(hyp, tb_writer, opt, device):\n",
        "    print(f'Hyperparameters {hyp}')\n",
        "    log_dir = tb_writer.log_dir if tb_writer else 'runs/evolution'  # run directory\n",
        "    wdir = str(Path(log_dir) / 'weights') + os.sep  # weights directory\n",
        "    os.makedirs(wdir, exist_ok=True)\n",
        "    last = wdir + 'last.pt'\n",
        "    best = wdir + 'best.pt'\n",
        "    results_file = log_dir + os.sep + 'results.txt'\n",
        "    epochs, batch_size, total_batch_size, weights, rank = \\\n",
        "        opt.epochs, opt.batch_size, opt.total_batch_size, opt.weights, opt.local_rank\n",
        "    # TODO: Init DDP logging. Only the first process is allowed to log.\n",
        "    # Since I see lots of print here, the logging configuration is skipped here. We may see repeated outputs.\n",
        "\n",
        "    # Save run settings\n",
        "    with open(Path(log_dir) / 'hyp.yaml', 'w') as f:\n",
        "        yaml.dump(hyp, f, sort_keys=False)\n",
        "    with open(Path(log_dir) / 'opt.yaml', 'w') as f:\n",
        "        yaml.dump(vars(opt), f, sort_keys=False)\n",
        "\n",
        "    # Configure\n",
        "    init_seeds(2 + rank)\n",
        "    with open(opt.data) as f:\n",
        "        data_dict = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
        "    train_path = data_dict['train']\n",
        "    test_path = data_dict['val']\n",
        "    nc, names = (1, ['item']) if opt.single_cls else (int(data_dict['nc']), data_dict['names'])  # number classes, names\n",
        "    assert len(names) == nc, '%g names found for nc=%g dataset in %s' % (len(names), nc, opt.data)  # check\n",
        "\n",
        "    # Remove previous results\n",
        "    if rank in [-1, 0]:\n",
        "        for f in glob.glob('*_batch*.jpg') + glob.glob(results_file):\n",
        "            os.remove(f)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(opt.cfg, nc=nc).to(device)\n",
        "\n",
        "    # Image sizes\n",
        "    gs = int(max(model.stride))  # grid size (max stride)\n",
        "    imgsz, imgsz_test = [check_img_size(x, gs) for x in opt.img_size]  # verify imgsz are gs-multiples\n",
        "\n",
        "    # Optimizer\n",
        "    nbs = 64  # nominal batch size\n",
        "   \n",
        "    accumulate = max(round(nbs / total_batch_size), 1)  # accumulate loss before optimizing\n",
        "    hyp['weight_decay'] *= total_batch_size * accumulate / nbs  # scale weight_decay\n",
        "\n",
        "    pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n",
        "    for k, v in model.named_parameters():\n",
        "        if v.requires_grad:\n",
        "            if '.bias' in k:\n",
        "                pg2.append(v)  # biases\n",
        "            elif '.weight' in k and '.bn' not in k:\n",
        "                pg1.append(v)  # apply weight decay\n",
        "            else:\n",
        "                pg0.append(v)  # all else\n",
        "\n",
        "    if hyp['optimizer'] == 'adam': \n",
        "        optimizer = optim.Adam(pg0, lr=hyp['lr0'], betas=(hyp['momentum'], 0.999))  # adjust beta1 to momentum\n",
        "    else:\n",
        "        optimizer = optim.SGD(pg0, lr=hyp['lr0'], momentum=hyp['momentum'], nesterov=True)\n",
        "\n",
        "    optimizer.add_param_group({'params': pg1, 'weight_decay': hyp['weight_decay']})  # add pg1 with weight_decay\n",
        "    optimizer.add_param_group({'params': pg2})  # add pg2 (biases)\n",
        "    print('Optimizer groups: %g .bias, %g conv.weight, %g other' % (len(pg2), len(pg1), len(pg0)))\n",
        "    del pg0, pg1, pg2\n",
        "\n",
        "    # Load Model\n",
        "    with torch_distributed_zero_first(rank):\n",
        "        google_utils.attempt_download(weights)\n",
        "    start_epoch, best_fitness = 0, 0.0\n",
        "    if weights.endswith('.pt'):  # pytorch format\n",
        "        ckpt = torch.load(weights, map_location=device)  # load checkpoint\n",
        "\n",
        "        # load model\n",
        "        try:\n",
        "            exclude = ['anchor']  # exclude keys\n",
        "            ckpt['model'] = {k: v for k, v in ckpt['model'].float().state_dict().items()\n",
        "                             if k in model.state_dict() and not any(x in k for x in exclude)\n",
        "                             and model.state_dict()[k].shape == v.shape}\n",
        "            model.load_state_dict(ckpt['model'], strict=False)\n",
        "            print('Transferred %g/%g items from %s' % (len(ckpt['model']), len(model.state_dict()), weights))\n",
        "        except KeyError as e:\n",
        "            s = \"%s is not compatible with %s. This may be due to model differences or %s may be out of date. \" \\\n",
        "                \"Please delete or update %s and try again, or use --weights '' to train from scratch.\" \\\n",
        "                % (weights, opt.cfg, weights, weights)\n",
        "            raise KeyError(s) from e\n",
        "\n",
        "        # load optimizer\n",
        "        if ckpt['optimizer'] is not None:\n",
        "            optimizer.load_state_dict(ckpt['optimizer'])\n",
        "            best_fitness = ckpt['best_fitness']\n",
        "\n",
        "        # load results\n",
        "        if ckpt.get('training_results') is not None:\n",
        "            with open(results_file, 'w') as file:\n",
        "                file.write(ckpt['training_results'])  # write results.txt\n",
        "\n",
        "        # epochs\n",
        "        start_epoch = ckpt['epoch'] + 1\n",
        "        if epochs < start_epoch:\n",
        "            print('%s has been trained for %g epochs. Fine-tuning for %g additional epochs.' %\n",
        "                  (weights, ckpt['epoch'], epochs))\n",
        "            epochs += ckpt['epoch']  # finetune additional epochs\n",
        "\n",
        "        del ckpt\n",
        "\n",
        "    if mixed_precision:\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
        "\n",
        "    lf = lambda x: (((1 + math.cos(x * math.pi / epochs)) / 2) ** 1.0) * 0.9 + 0.1  # cosine\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
        "    \n",
        "\n",
        "    # DP mode\n",
        "    if device.type != 'cpu' and rank == -1 and torch.cuda.device_count() > 1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    # SyncBatchNorm\n",
        "    if opt.sync_bn and device.type != 'cpu' and rank != -1:\n",
        "        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).to(device)\n",
        "        print('Using SyncBatchNorm()')\n",
        "\n",
        "    # Exponential moving average\n",
        "    ema = torch_utils.ModelEMA(model) if rank in [-1, 0] else None\n",
        "\n",
        "    # DDP mode\n",
        "    if device.type != 'cpu' and rank != -1:\n",
        "        model = DDP(model, device_ids=[rank], output_device=rank)\n",
        "\n",
        "    # Trainloader\n",
        "    dataloader, dataset = create_dataloader(train_path, imgsz, batch_size, gs, opt, hyp=hyp, augment=True,\n",
        "                                            cache=opt.cache_images, rect=opt.rect, local_rank=rank,\n",
        "                                            world_size=opt.world_size)\n",
        "    mlc = np.concatenate(dataset.labels, 0)[:, 0].max()  # max label class\n",
        "    nb = len(dataloader)  # number of batches\n",
        "    assert mlc < nc, 'Label class %g exceeds nc=%g in %s. Possible class labels are 0-%g' % (mlc, nc, opt.data, nc - 1)\n",
        "\n",
        "    # Testloader\n",
        "    if rank in [-1, 0]:\n",
        "        # local_rank is set to -1. Because only the first process is expected to do evaluation.\n",
        "        testloader = create_dataloader(test_path, imgsz_test, total_batch_size, gs, opt, hyp=hyp, augment=False,\n",
        "                                       cache=opt.cache_images, rect=True, local_rank=-1, world_size=opt.world_size)[0]\n",
        "\n",
        "    # Model parameters\n",
        "    hyp['cls'] *= nc / 80.  # scale coco-tuned hyp['cls'] to current dataset\n",
        "    model.nc = nc  # attach number of classes to model\n",
        "    model.hyp = hyp  # attach hyperparameters to model\n",
        "    model.gr = 1.0  # giou loss ratio (obj_loss = 1.0 or giou)\n",
        "    model.class_weights = labels_to_class_weights(dataset.labels, nc).to(device)  # attach class weights\n",
        "    model.names = names\n",
        "\n",
        "    # Class frequency\n",
        "    if rank in [-1, 0]:\n",
        "        labels = np.concatenate(dataset.labels, 0)\n",
        "        c = torch.tensor(labels[:, 0])  # classes\n",
        "        # cf = torch.bincount(c.long(), minlength=nc) + 1.\n",
        "        # model._initialize_biases(cf.to(device))\n",
        "        plot_labels(labels, save_dir=log_dir)\n",
        "        if tb_writer:\n",
        "            # tb_writer.add_hparams(hyp, {}) \n",
        "            tb_writer.add_histogram('classes', c, 0)\n",
        "\n",
        "        # Check anchors\n",
        "        if not opt.noautoanchor:\n",
        "            check_anchors(dataset, model=model, thr=hyp['anchor_t'], imgsz=imgsz)\n",
        "\n",
        "    # Start training\n",
        "    t0 = time.time()\n",
        "    nw = max(3 * nb, 1e3)  # number of warmup iterations, max(3 epochs, 1k iterations)\n",
        "    maps = np.zeros(nc)  # mAP per class\n",
        "    results = (0, 0, 0, 0, 0, 0, 0)  \n",
        "    scheduler.last_epoch = start_epoch - 1  # do not move\n",
        "    if rank in [0, -1]:\n",
        "        print('Image sizes %g train, %g test' % (imgsz, imgsz_test))\n",
        "        print('Using %g dataloader workers' % dataloader.num_workers)\n",
        "        print('Starting training for %g epochs...' % epochs)\n",
        "    # torch.autograd.set_detect_anomaly(True)\n",
        "    for epoch in range(start_epoch, epochs):  \n",
        "        model.train()\n",
        "\n",
        "\n",
        "        if dataset.image_weights:\n",
        "            # Generate indices.\n",
        "            if rank in [-1, 0]:\n",
        "                w = model.class_weights.cpu().numpy() * (1 - maps) ** 2  # class weights\n",
        "                image_weights = labels_to_image_weights(dataset.labels, nc=nc, class_weights=w)\n",
        "                dataset.indices = random.choices(range(dataset.n), weights=image_weights,\n",
        "                                                 k=dataset.n)  # rand weighted idx\n",
        "            # Broadcast.\n",
        "            if rank != -1:\n",
        "                indices = torch.zeros([dataset.n], dtype=torch.int)\n",
        "                if rank == 0:\n",
        "                    indices[:] = torch.from_tensor(dataset.indices, dtype=torch.int)\n",
        "                dist.broadcast(indices, 0)\n",
        "                if rank != 0:\n",
        "                    dataset.indices = indices.cpu().numpy()\n",
        "\n",
        " \n",
        "\n",
        "        mloss = torch.zeros(4, device=device)  # mean losses\n",
        "        if rank != -1:\n",
        "            dataloader.sampler.set_epoch(epoch)\n",
        "        pbar = enumerate(dataloader)\n",
        "        if rank in [-1, 0]:\n",
        "            print(('\\n' + '%10s' * 8) % ('Epoch', 'gpu_mem', 'GIoU', 'obj', 'cls', 'total', 'targets', 'img_size'))\n",
        "            pbar = tqdm(pbar, total=nb)  # progress bar\n",
        "        optimizer.zero_grad()\n",
        "        for i, (imgs, targets, paths, _) in pbar:  # batch -------------------------------------------------------------\n",
        "            ni = i + nb * epoch  # number integrated batches (since train start)\n",
        "            imgs = imgs.to(device, non_blocking=True).float() / 255.0  \n",
        "\n",
        "            # Warmup\n",
        "            if ni <= nw:\n",
        "                xi = [0, nw]  # x interp\n",
        "            \n",
        "                accumulate = max(1, np.interp(ni, xi, [1, nbs / total_batch_size]).round())\n",
        "                for j, x in enumerate(optimizer.param_groups):\n",
        "                   \n",
        "                    x['lr'] = np.interp(ni, xi, [0.1 if j == 2 else 0.0, x['initial_lr'] * lf(epoch)])\n",
        "                    if 'momentum' in x:\n",
        "                        x['momentum'] = np.interp(ni, xi, [0.9, hyp['momentum']])\n",
        "\n",
        "            # Multi-scale\n",
        "            if opt.multi_scale:\n",
        "                sz = random.randrange(imgsz * 0.5, imgsz * 1.5 + gs) // gs * gs  # size\n",
        "                sf = sz / max(imgs.shape[2:])  # scale factor\n",
        "                if sf != 1:\n",
        "                    ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n",
        "                    imgs = F.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n",
        "\n",
        "            # Forward\n",
        "            pred = model(imgs)\n",
        "\n",
        "            # Loss\n",
        "            loss, loss_items = compute_loss(pred, targets.to(device), model)  # scaled by batch_size\n",
        "            if rank != -1:\n",
        "                loss *= opt.world_size  \n",
        "            if not torch.isfinite(loss):\n",
        "                print('WARNING: non-finite loss, ending training ', loss_items)\n",
        "                return results\n",
        "\n",
        "            # Backward\n",
        "            if mixed_precision:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            # Optimize\n",
        "            if ni % accumulate == 0:\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                if ema is not None:\n",
        "                    ema.update(model)\n",
        "\n",
        "            # Print\n",
        "            if rank in [-1, 0]:\n",
        "                mloss = (mloss * i + loss_items) / (i + 1)  # update mean losses\n",
        "                mem = '%.3gG' % (torch.cuda.memory_cached() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n",
        "                s = ('%10s' * 2 + '%10.4g' * 6) % (\n",
        "                    '%g/%g' % (epoch, epochs - 1), mem, *mloss, targets.shape[0], imgs.shape[-1])\n",
        "                pbar.set_description(s)\n",
        "\n",
        "                # Plot\n",
        "                if ni < 3:\n",
        "                    f = str(Path(log_dir) / ('train_batch%g.jpg' % ni))  # filename\n",
        "                    result = plot_images(images=imgs, targets=targets, paths=paths, fname=f)\n",
        "                    if tb_writer and result is not None:\n",
        "                        tb_writer.add_image(f, result, dataformats='HWC', global_step=epoch)\n",
        "                        \n",
        "            # end batch ------------------------------------------------------------------------------------------------\n",
        "\n",
        "        # Scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "   \n",
        "        if rank in [-1, 0]:\n",
        "            # mAP\n",
        "            if ema is not None:\n",
        "                ema.update_attr(model, include=['md', 'nc', 'hyp', 'gr', 'names', 'stride'])\n",
        "            final_epoch = epoch + 1 == epochs\n",
        "            if not opt.notest or final_epoch:  # Calculate mAP\n",
        "                results, maps, times = test.test(opt.data,\n",
        "                                                 batch_size=total_batch_size,\n",
        "                                                 imgsz=imgsz_test,\n",
        "                                                 save_json=final_epoch and opt.data.endswith(os.sep + 'coco.yaml'),\n",
        "                                                 model=ema.ema.module if hasattr(ema.ema, 'module') else ema.ema,\n",
        "                                                 single_cls=opt.single_cls,\n",
        "                                                 dataloader=testloader,\n",
        "                                                 save_dir=log_dir)\n",
        "\n",
        "                # Write\n",
        "                with open(results_file, 'a') as f:\n",
        "                    f.write(s + '%10.4g' * 7 % results + '\\n')  \n",
        "                if len(opt.name) and opt.bucket:\n",
        "                    os.system('gsutil cp %s gs://%s/results/results%s.txt' % (results_file, opt.bucket, opt.name))\n",
        "\n",
        "                # Tensorboard\n",
        "                if tb_writer:\n",
        "                    tags = ['train/giou_loss', 'train/obj_loss', 'train/cls_loss',\n",
        "                            'metrics/precision', 'metrics/recall', 'metrics/mAP_0.5', 'metrics/mAP_0.5:0.95',\n",
        "                            'val/giou_loss', 'val/obj_loss', 'val/cls_loss']\n",
        "                    for x, tag in zip(list(mloss[:-1]) + list(results), tags):\n",
        "                        tb_writer.add_scalar(tag, x, epoch)\n",
        "\n",
        "                # Update best mAP\n",
        "                fi = fitness(np.array(results).reshape(1, -1)) \n",
        "                if fi > best_fitness:\n",
        "                    best_fitness = fi\n",
        "\n",
        "            # Save model\n",
        "            save = (not opt.nosave) or (final_epoch and not opt.evolve)\n",
        "            if save:\n",
        "                with open(results_file, 'r') as f:  # create checkpoint\n",
        "                    ckpt = {'epoch': epoch,\n",
        "                            'best_fitness': best_fitness,\n",
        "                            'training_results': f.read(),\n",
        "                            'model': ema.ema.module if hasattr(ema, 'module') else ema.ema,\n",
        "                            'optimizer': None if final_epoch else optimizer.state_dict()}\n",
        "\n",
        "                # Save last, best and delete\n",
        "                torch.save(ckpt, last)\n",
        "                if (best_fitness == fi) and not final_epoch:\n",
        "                    torch.save(ckpt, best)\n",
        "                del ckpt\n",
        "      \n",
        "\n",
        "    if rank in [-1, 0]:\n",
        "        # Strip optimizers\n",
        "        n = ('_' if len(opt.name) and not opt.name.isnumeric() else '') + opt.name\n",
        "        fresults, flast, fbest = 'results%s.txt' % n, wdir + 'last%s.pt' % n, wdir + 'best%s.pt' % n\n",
        "        for f1, f2 in zip([wdir + 'last.pt', wdir + 'best.pt', 'results.txt'], [flast, fbest, fresults]):\n",
        "            if os.path.exists(f1):\n",
        "                os.rename(f1, f2)  # rename\n",
        "                ispt = f2.endswith('.pt')  # is *.pt\n",
        "                strip_optimizer(f2) if ispt else None  # strip optimizer\n",
        "                os.system('gsutil cp %s gs://%s/weights' % (f2, opt.bucket)) if opt.bucket and ispt else None  # upload\n",
        "        # Finish\n",
        "        if not opt.evolve:\n",
        "            plot_results(save_dir=log_dir)  # save as results.png\n",
        "        print('%g epochs completed in %.3f hours.\\n' % (epoch - start_epoch + 1, (time.time() - t0) / 3600))\n",
        "\n",
        "    dist.destroy_process_group() if rank not in [-1, 0] else None\n",
        "    torch.cuda.empty_cache()\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--cfg', type=str, default='models/yolov5s.yaml', help='model.yaml path')\n",
        "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='data.yaml path')\n",
        "    parser.add_argument('--hyp', type=str, default='', help='hyp.yaml path (optional)')\n",
        "    parser.add_argument('--epochs', type=int, default=300)\n",
        "    parser.add_argument('--batch-size', type=int, default=16, help=\"Total batch size for all gpus.\")\n",
        "    parser.add_argument('--img-size', nargs='+', type=int, default=[640, 640], help='train,test sizes')\n",
        "    parser.add_argument('--rect', action='store_true', help='rectangular training')\n",
        "    parser.add_argument('--resume', nargs='?', const='get_last', default=False,\n",
        "                        help='resume from given path/to/last.pt, or most recent run if blank.')\n",
        "    parser.add_argument('--nosave', action='store_true', help='only save final checkpoint')\n",
        "    parser.add_argument('--notest', action='store_true', help='only test final epoch')\n",
        "    parser.add_argument('--noautoanchor', action='store_true', help='disable autoanchor check')\n",
        "    parser.add_argument('--evolve', action='store_true', help='evolve hyperparameters')\n",
        "    parser.add_argument('--bucket', type=str, default='', help='gsutil bucket')\n",
        "    parser.add_argument('--cache-images', action='store_true', help='cache images for faster training')\n",
        "    parser.add_argument('--weights', type=str, default='', help='initial weights path')\n",
        "    parser.add_argument('--name', default='', help='renames results.txt to results_name.txt if supplied')\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--multi-scale', action='store_true', help='vary img-size +/- 50%%')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='train as single-class dataset')\n",
        "    parser.add_argument('--sync-bn', action='store_true', help='use SyncBatchNorm, only available in DDP mode')\n",
        "    parser.add_argument('--local_rank', type=int, default=-1, help='DDP parameter, do not modify')\n",
        "    opt = parser.parse_args()\n",
        "\n",
        "    last = get_latest_run() if opt.resume == 'get_last' else opt.resume  # resume from most recent run\n",
        "    if last and not opt.weights:\n",
        "        print(f'Resuming training from {last}')\n",
        "    opt.weights = last if opt.resume and not opt.weights else opt.weights\n",
        "    if opt.local_rank in [-1, 0]:\n",
        "        check_git_status()\n",
        "    opt.cfg = check_file(opt.cfg)  # check file\n",
        "    opt.data = check_file(opt.data)  # check file\n",
        "    if opt.hyp:  # update hyps\n",
        "        opt.hyp = check_file(opt.hyp)  # check file\n",
        "        with open(opt.hyp) as f:\n",
        "            hyp.update(yaml.load(f, Loader=yaml.FullLoader))  # update hyps\n",
        "    opt.img_size.extend([opt.img_size[-1]] * (2 - len(opt.img_size)))  # extend to 2 sizes (train, test)\n",
        "    device = torch_utils.select_device(opt.device, apex=mixed_precision, batch_size=opt.batch_size)\n",
        "    opt.total_batch_size = opt.batch_size\n",
        "    opt.world_size = 1\n",
        "    if device.type == 'cpu':\n",
        "        mixed_precision = False\n",
        "    elif opt.local_rank != -1:\n",
        "        # DDP mode\n",
        "        assert torch.cuda.device_count() > opt.local_rank\n",
        "        torch.cuda.set_device(opt.local_rank)\n",
        "        device = torch.device(\"cuda\", opt.local_rank)\n",
        "        dist.init_process_group(backend='nccl', init_method='env://')  # distributed backend\n",
        "\n",
        "        opt.world_size = dist.get_world_size()\n",
        "        assert opt.batch_size % opt.world_size == 0, \"Batch size is not a multiple of the number of devices given!\"\n",
        "        opt.batch_size = opt.total_batch_size // opt.world_size\n",
        "    print(opt)\n",
        "\n",
        "    # Train\n",
        "    if not opt.evolve:\n",
        "        if opt.local_rank in [-1, 0]:\n",
        "            print('Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/')\n",
        "            tb_writer = SummaryWriter(log_dir=increment_dir('runs/exp', opt.name))\n",
        "        else:\n",
        "            tb_writer = None\n",
        "        train(hyp, tb_writer, opt, device)\n",
        "\n",
        "    # Evolve hyperparameters (optional)\n",
        "    else:\n",
        "        assert opt.local_rank == -1, \"DDP mode currently not implemented for Evolve!\"\n",
        "\n",
        "        tb_writer = None\n",
        "        opt.notest, opt.nosave = True, True  # only test/save final epoch\n",
        "        if opt.bucket:\n",
        "            os.system('gsutil cp gs://%s/evolve.txt .' % opt.bucket)  # download evolve.txt if exists\n",
        "\n",
        "        for _ in range(10):  # generations to evolve\n",
        "            if os.path.exists('evolve.txt'):  # if evolve.txt exists: select best hyps and mutate\n",
        "                # Select parent(s)\n",
        "                parent = 'single'  # parent selection method: 'single' or 'weighted'\n",
        "                x = np.loadtxt('evolve.txt', ndmin=2)\n",
        "                n = min(5, len(x))  # number of previous results to consider\n",
        "                x = x[np.argsort(-fitness(x))][:n]  # top n mutations\n",
        "                w = fitness(x) - fitness(x).min()  # weights\n",
        "                if parent == 'single' or len(x) == 1:\n",
        "                    # x = x[random.randint(0, n - 1)]  # random selection\n",
        "                    x = x[random.choices(range(n), weights=w)[0]]  # weighted selection\n",
        "                elif parent == 'weighted':\n",
        "                    x = (x * w.reshape(n, 1)).sum(0) / w.sum()  # weighted combination\n",
        "\n",
        "                # Mutate\n",
        "                mp, s = 0.9, 0.2  # mutation probability, sigma\n",
        "                npr = np.random\n",
        "                npr.seed(int(time.time()))\n",
        "                g = np.array([1, 1, 1, 1, 1, 1, 1, 0, .1, 1, 0, 1, 1, 1, 1, 1, 1, 1])  # gains\n",
        "                ng = len(g)\n",
        "                v = np.ones(ng)\n",
        "                while all(v == 1):  # mutate until a change occurs (prevent duplicates)\n",
        "                    v = (g * (npr.random(ng) < mp) * npr.randn(ng) * npr.random() * s + 1).clip(0.3, 3.0)\n",
        "                for i, k in enumerate(hyp.keys()):  # plt.hist(v.ravel(), 300)\n",
        "                    hyp[k] = x[i + 7] * v[i]  # mutate\n",
        "\n",
        "            # Clip to limits\n",
        "            keys = ['lr0', 'iou_t', 'momentum', 'weight_decay', 'hsv_s', 'hsv_v', 'translate', 'scale', 'fl_gamma']\n",
        "            limits = [(1e-5, 1e-2), (0.00, 0.70), (0.60, 0.98), (0, 0.001), (0, .9), (0, .9), (0, .9), (0, .9), (0, 3)]\n",
        "            for k, v in zip(keys, limits):\n",
        "                hyp[k] = np.clip(hyp[k], v[0], v[1])\n",
        "\n",
        "            # Train mutation\n",
        "            results = train(hyp.copy(), tb_writer, opt, device)\n",
        "\n",
        "            # Write mutation results\n",
        "            print_mutation(hyp, results, opt.bucket)\n",
        "\n",
        "            # Plot results\n",
        "            # plot_evolution_results(hyp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU9XwGxlpC3F"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "\n",
        "from models.experimental import *\n",
        "from utils.datasets import *\n",
        "\n",
        "\n",
        "def test(data,\n",
        "         weights=None,\n",
        "         batch_size=16,\n",
        "         imgsz=640,\n",
        "         conf_thres=0.001,\n",
        "         iou_thres=0.6,  # for NMS\n",
        "         save_json=False,\n",
        "         single_cls=False,\n",
        "         augment=False,\n",
        "         verbose=False,\n",
        "         model=None,\n",
        "         dataloader=None,\n",
        "         save_dir='',\n",
        "         merge=False,\n",
        "         save_txt=False):\n",
        "    # Initialize/load model and set device\n",
        "    training = model is not None\n",
        "    if training:  # called by train.py\n",
        "        device = next(model.parameters()).device  # get model device\n",
        "\n",
        "    else:  # called directly\n",
        "        device = torch_utils.select_device(opt.device, batch_size=batch_size)\n",
        "        merge, save_txt = opt.merge, opt.save_txt  # use Merge NMS, save *.txt labels\n",
        "        if save_txt:\n",
        "            out = Path('inference/output')\n",
        "            if os.path.exists(out):\n",
        "                shutil.rmtree(out)  # delete output folder\n",
        "            os.makedirs(out)  # make new output folder\n",
        "\n",
        "        # Remove previous\n",
        "        for f in glob.glob(str(Path(save_dir) / 'test_batch*.jpg')):\n",
        "            os.remove(f)\n",
        "\n",
        "        # Load model\n",
        "        model = attempt_load(weights, map_location=device)  # load FP32 model\n",
        "        imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
        "\n",
        "\n",
        "\n",
        "    # Half\n",
        "    half = device.type != 'cpu'  # half precision only supported on CUDA\n",
        "    if half:\n",
        "        model.half()\n",
        "\n",
        "    # Configure\n",
        "    model.eval()\n",
        "    with open(data) as f:\n",
        "        data = yaml.load(f, Loader=yaml.FullLoader)  # model dict\n",
        "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
        "    iouv = torch.linspace(0.5, 0.95, 10).to(device)  # iou vector for mAP@0.5:0.95\n",
        "    niou = iouv.numel()\n",
        "\n",
        "    # Dataloader\n",
        "    if not training:\n",
        "        img = torch.zeros((1, 3, imgsz, imgsz), device=device)  # init img\n",
        "        _ = model(img.half() if half else img) if device.type != 'cpu' else None  # run once\n",
        "        path = data['test'] if opt.task == 'test' else data['val']  # path to val/test images\n",
        "        dataloader = create_dataloader(path, imgsz, batch_size, model.stride.max(), opt,\n",
        "                                       hyp=None, augment=False, cache=False, pad=0.5, rect=True)[0]\n",
        "\n",
        "    seen = 0\n",
        "    names = model.names if hasattr(model, 'names') else model.module.names\n",
        "    coco91class = coco80_to_coco91_class()\n",
        "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Targets', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
        "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
        "    loss = torch.zeros(3, device=device)\n",
        "    jdict, stats, ap, ap_class = [], [], [], []\n",
        "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
        "        img = img.to(device, non_blocking=True)\n",
        "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        targets = targets.to(device)\n",
        "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
        "        whwh = torch.Tensor([width, height, width, height]).to(device)\n",
        "\n",
        "        # Disable gradients\n",
        "        with torch.no_grad():\n",
        "            # Run model\n",
        "            t = torch_utils.time_synchronized()\n",
        "            inf_out, train_out = model(img, augment=augment)  # inference and training outputs\n",
        "            t0 += torch_utils.time_synchronized() - t\n",
        "\n",
        "            # Compute loss\n",
        "            if training:  # if model has loss hyperparameters\n",
        "                loss += compute_loss([x.float() for x in train_out], targets, model)[1][:3]  # GIoU, obj, cls\n",
        "\n",
        "            # Run NMS\n",
        "            t = torch_utils.time_synchronized()\n",
        "            output = non_max_suppression(inf_out, conf_thres=conf_thres, iou_thres=iou_thres, merge=merge)\n",
        "            t1 += torch_utils.time_synchronized() - t\n",
        "\n",
        "        # Statistics per image\n",
        "        for si, pred in enumerate(output):\n",
        "            labels = targets[targets[:, 0] == si, 1:]\n",
        "            nl = len(labels)\n",
        "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
        "            seen += 1\n",
        "\n",
        "            if pred is None:\n",
        "                if nl:\n",
        "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
        "                continue\n",
        "\n",
        "            # Append to text file\n",
        "            if save_txt:\n",
        "                gn = torch.tensor(shapes[si][0])[[1, 0, 1, 0]]  # normalization gain whwh\n",
        "                txt_path = str(out / Path(paths[si]).stem)\n",
        "                pred[:, :4] = scale_coords(img[si].shape[1:], pred[:, :4], shapes[si][0], shapes[si][1])  # to original\n",
        "                for *xyxy, conf, cls in pred:\n",
        "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
        "                    with open(txt_path + '.txt', 'a') as f:\n",
        "                        f.write(('%g ' * 5 + '\\n') % (cls, *xywh))  # label format\n",
        "\n",
        "            # Clip boxes to image bounds\n",
        "            clip_coords(pred, (height, width))\n",
        "\n",
        "            # Append to pycocotools JSON dictionary\n",
        "            if save_json:\n",
        "                image_id = Path(paths[si]).stem\n",
        "                box = pred[:, :4].clone()  # xyxy\n",
        "                scale_coords(img[si].shape[1:], box, shapes[si][0], shapes[si][1])  # to original shape\n",
        "                box = xyxy2xywh(box)  # xywh\n",
        "                box[:, :2] -= box[:, 2:] / 2  # xy center to top-left corner\n",
        "                for p, b in zip(pred.tolist(), box.tolist()):\n",
        "                    jdict.append({'image_id': int(image_id) if image_id.isnumeric() else image_id,\n",
        "                                  'category_id': coco91class[int(p[5])],\n",
        "                                  'bbox': [round(x, 3) for x in b],\n",
        "                                  'score': round(p[4], 5)})\n",
        "\n",
        "            # Assign all predictions as incorrect\n",
        "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool, device=device)\n",
        "            if nl:\n",
        "                detected = []  # target indices\n",
        "                tcls_tensor = labels[:, 0]\n",
        "\n",
        "                # target boxes\n",
        "                tbox = xywh2xyxy(labels[:, 1:5]) * whwh\n",
        "\n",
        "                # Per target class\n",
        "                for cls in torch.unique(tcls_tensor):\n",
        "                    ti = (cls == tcls_tensor).nonzero().view(-1)  # prediction indices\n",
        "                    pi = (cls == pred[:, 5]).nonzero().view(-1)  # target indices\n",
        "\n",
        "                    # Search for detections\n",
        "                    if pi.shape[0]:\n",
        "                        # Prediction to target ious\n",
        "                        ious, i = box_iou(pred[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
        "\n",
        "                        # Append detections\n",
        "                        for j in (ious > iouv[0]).nonzero():\n",
        "                            d = ti[i[j]]  # detected target\n",
        "                            if d not in detected:\n",
        "                                detected.append(d)\n",
        "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
        "                                if len(detected) == nl:  # all targets already located in image\n",
        "                                    break\n",
        "\n",
        "            # Append statistics (correct, conf, pcls, tcls)\n",
        "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
        "\n",
        "        # Plot images\n",
        "        if batch_i < 1:\n",
        "            f = Path(save_dir) / ('test_batch%g_gt.jpg' % batch_i)  # filename\n",
        "            plot_images(img, targets, paths, str(f), names)  # ground truth\n",
        "            f = Path(save_dir) / ('test_batch%g_pred.jpg' % batch_i)\n",
        "            plot_images(img, output_to_target(output, width, height), paths, str(f), names)  # predictions\n",
        "\n",
        "    # Compute statistics\n",
        "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
        "    if len(stats) and stats[0].any():\n",
        "        p, r, ap, f1, ap_class = ap_per_class(*stats)\n",
        "        p, r, ap50, ap = p[:, 0], r[:, 0], ap[:, 0], ap.mean(1)  # [P, R, AP@0.5, AP@0.5:0.95]\n",
        "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
        "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
        "    else:\n",
        "        nt = torch.zeros(1)\n",
        "\n",
        "    # Print results\n",
        "    pf = '%20s' + '%12.3g' * 6  # print format\n",
        "    print(pf % ('all', seen, nt.sum(), mp, mr, map50, map))\n",
        "\n",
        "    # Print results per class\n",
        "    if verbose and nc > 1 and len(stats):\n",
        "        for i, c in enumerate(ap_class):\n",
        "            print(pf % (names[c], seen, nt[c], p[i], r[i], ap50[i], ap[i]))\n",
        "\n",
        "    # Print speeds\n",
        "    t = tuple(x / seen * 1E3 for x in (t0, t1, t0 + t1)) + (imgsz, imgsz, batch_size)  # tuple\n",
        "    if not training:\n",
        "        print('Speed: %.1f/%.1f/%.1f ms inference/NMS/total per %gx%g image at batch-size %g' % t)\n",
        "\n",
        "    # Save JSON\n",
        "    if save_json and len(jdict):\n",
        "        f = 'detections_val2017_%s_results.json' % \\\n",
        "            (weights.split(os.sep)[-1].replace('.pt', '') if isinstance(weights, str) else '')  # filename\n",
        "        print('\\nCOCO mAP with pycocotools... saving %s...' % f)\n",
        "        with open(f, 'w') as file:\n",
        "            json.dump(jdict, file)\n",
        "\n",
        "       \n",
        "            from pycocotools.coco import COCO\n",
        "            from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "            imgIds = [int(Path(x).stem) for x in dataloader.dataset.img_files]\n",
        "            cocoGt = COCO(glob.glob('../coco/annotations/instances_val*.json')[0])  # initialize COCO ground truth api\n",
        "            cocoDt = cocoGt.loadRes(f)  # initialize COCO pred api\n",
        "            cocoEval = COCOeval(cocoGt, cocoDt, 'bbox')\n",
        "            cocoEval.params.imgIds = imgIds  # image IDs to evaluate\n",
        "            cocoEval.evaluate()\n",
        "            cocoEval.accumulate()\n",
        "            cocoEval.summarize()\n",
        "            map, map50 = cocoEval.stats[:2]  # update results (mAP@0.5:0.95, mAP@0.5)\n",
        "        except Exception as e:\n",
        "            print('ERROR: pycocotools unable to run: %s' % e)\n",
        "\n",
        "    # Return results\n",
        "    model.float()  # for training\n",
        "    maps = np.zeros(nc) + map\n",
        "    for i, c in enumerate(ap_class):\n",
        "        maps[c] = ap[i]\n",
        "    return (mp, mr, map50, map, *(loss.cpu() / len(dataloader)).tolist()), maps, t\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(prog='test.py')\n",
        "    parser.add_argument('--weights', nargs='+', type=str, default='yolov5s.pt', help='model.pt path(s)')\n",
        "    parser.add_argument('--data', type=str, default='data/coco128.yaml', help='*.data path')\n",
        "    parser.add_argument('--batch-size', type=int, default=32, help='size of each image batch')\n",
        "    parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "    parser.add_argument('--conf-thres', type=float, default=0.001, help='object confidence threshold')\n",
        "    parser.add_argument('--iou-thres', type=float, default=0.65, help='IOU threshold for NMS')\n",
        "    parser.add_argument('--save-json', action='store_true', help='save a cocoapi-compatible JSON results file')\n",
        "    parser.add_argument('--task', default='val', help=\"'val', 'test', 'study'\")\n",
        "    parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "    parser.add_argument('--single-cls', action='store_true', help='treat as single-class dataset')\n",
        "    parser.add_argument('--augment', action='store_true', help='augmented inference')\n",
        "    parser.add_argument('--merge', action='store_true', help='use Merge NMS')\n",
        "    parser.add_argument('--verbose', action='store_true', help='report mAP by class')\n",
        "    parser.add_argument('--save-txt', action='store_true', help='save results to *.txt')\n",
        "    opt = parser.parse_args()\n",
        "    opt.save_json |= opt.data.endswith('coco.yaml')\n",
        "    opt.data = check_file(opt.data)  # check file\n",
        "    print(opt)\n",
        "\n",
        "    if opt.task in ['val', 'test']:  # run normally\n",
        "        test(opt.data,\n",
        "             opt.weights,\n",
        "             opt.batch_size,\n",
        "             opt.img_size,\n",
        "             opt.conf_thres,\n",
        "             opt.iou_thres,\n",
        "             opt.save_json,\n",
        "             opt.single_cls,\n",
        "             opt.augment,\n",
        "             opt.verbose)\n",
        "\n",
        "    elif opt.task == 'study':  # run over a range of settings and save/plot\n",
        "        for weights in ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt', 'yolov3-spp.pt']:\n",
        "            f = 'study_%s_%s.txt' % (Path(opt.data).stem, Path(weights).stem)  # filename to save to\n",
        "            x = list(range(352, 832, 64))  # x axis\n",
        "            y = []  # y axis\n",
        "            for i in x:  # img-size\n",
        "                print('\\nRunning %s point %s...' % (f, i))\n",
        "                r, _, t = test(opt.data, weights, opt.batch_size, i, opt.conf_thres, opt.iou_thres, opt.save_json)\n",
        "                y.append(r + t)  # results and times\n",
        "            np.savetxt(f, y, fmt='%10.4g')  # save\n",
        "        os.system('zip -r study.zip study_*.txt')\n",
        "        # plot_study_txt(f, x)  # plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rM7YbzWLLZw",
        "outputId": "95b787b0-e157-4a6d-8468-441885a55f7b"
      },
      "source": [
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Setup complete. Using torch 1.9.0+cu102 (CPU)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2mbjOWDLq4l"
      },
      "source": [
        "%rm -rf runs\n",
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "922ff578fbd14f778461a6409eb67456",
            "d0aa424944624b8cb8d9c03459d189dd",
            "9ee8c91d23154526bef1183e878fc186",
            "3100eb5035614d64b4e3c8421557483a",
            "0e8d925cb68742eba1a8346fbf6bd051",
            "e433cb5f55f741679bee86816836d4d0",
            "673d7c6ef05749d1876c18b10ce5bc79",
            "0bb77fc0b37c45ef8d58ddb116843feb",
            "667c591204eb4042bc3db412a90f7428",
            "6a147476fe0b4baf8b1bc1619b342193",
            "2d9fb5e938ef40d6bc5b87c9db8f7b31"
          ]
        },
        "id": "O8Oxf8SgN2mH",
        "outputId": "da5b03ee-5c3b-42c4-af28-34a96d1e3041"
      },
      "source": [
        "# Download COCO128\n",
        "torch.hub.download_url_to_file('https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip', 'tmp.zip')\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "922ff578fbd14f778461a6409eb67456",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/6.66M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtOonxfbYQXu",
        "outputId": "87661334-b530-464a-d8d5-e44cdca30b2d"
      },
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --cache"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=coco128.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, entity=None, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, upload_dataset=False, bbox_interval=-1, save_period=-1, artifact_alias=latest, local_rank=-1, freeze=0\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
            "YOLOv5 🚀 v5.0-371-gf3e3f76 torch 1.9.0+cu102 CPU\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.2, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "2021-08-16 19:14:48.232496: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Don't visualize my results'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 1c9dnp0b.\n",
            "2021-08-16 19:15:09.506724: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Focus                     [3, 32, 3]                    \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1    656896  models.common.SPP                       [512, 512, [5, 9, 13]]        \n",
            "  9                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    229245  models.yolo.Detect                      [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model Summary: 283 layers, 7276605 parameters, 7276605 gradients, 17.1 GFLOPs\n",
            "\n",
            "Transferred 362/362 items from yolov5s.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 59 weight, 62 weight (no decay), 62 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mversion 1.0.3 required by YOLOv5, but version 0.1.12 is currently installed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/coco128/labels/train2017' images and labels...128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<00:00, 1638.00it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/coco128/labels/train2017.cache\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:00<00:00, 239.10it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/coco128/labels/train2017.cache' images and labels... 128 found, 0 missing, 2 empty, 0 corrupted: 100% 128/128 [00:00<?, ?it/s]\n",
            "  0% 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  14% 18/128 [00:00<00:00, 178.78it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  28% 36/128 [00:00<00:00, 151.08it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram):  41% 52/128 [00:00<00:00, 138.97it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram):  52% 67/128 [00:00<00:00, 130.08it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram):  63% 81/128 [00:00<00:00, 123.41it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram):  73% 94/128 [00:00<00:00, 113.31it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram):  83% 106/128 [00:00<00:00, 112.67it/s]\u001b[A\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100% 128/128 [00:01<00:00, 123.21it/s]\n",
            "Plotting labels... \n",
            "\n",
            "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 4.26, Best Possible Recall (BPR) = 0.9925\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to runs/train/exp\n",
            "Starting training for 3 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       0/2        0G   0.04423   0.06615   0.02097       243       640: 100% 8/8 [03:38<00:00, 27.27s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [01:01<00:00, 15.31s/it]\n",
            "                 all        128        929      0.694      0.568      0.642       0.42\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       1/2        0G   0.04611   0.06131   0.02026       180       640: 100% 8/8 [03:20<00:00, 25.06s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [01:02<00:00, 15.56s/it]\n",
            "                 all        128        929      0.694      0.565      0.655      0.424\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "       2/2        0G   0.04595   0.06889   0.02033       229       640: 100% 8/8 [03:25<00:00, 25.74s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 4/4 [01:02<00:00, 15.70s/it]\n",
            "                 all        128        929      0.712      0.564       0.66       0.43\n",
            "\n",
            "3 epochs completed in 0.228 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 14.8MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 14.8MB\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 381\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/yolov5/wandb/offline-run-20210816_191508-1c9dnp0b/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/yolov5/wandb/offline-run-20210816_191508-1c9dnp0b/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/box_loss 0.04595\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/obj_loss 0.06889\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/cls_loss 0.02033\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              metrics/precision 0.71221\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 metrics/recall 0.56366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                metrics/mAP_0.5 0.66029\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           metrics/mAP_0.5:0.95 0.42965\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   val/box_loss 0.04122\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   val/obj_loss 0.03937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   val/cls_loss 0.0132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          x/lr0 9e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          x/lr1 9e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          x/lr2 0.09779\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       _runtime 840\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     _timestamp 1629142148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          _step 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/box_loss ▁█▇\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/obj_loss ▅▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/cls_loss █▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      metrics/precision ▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         metrics/recall █▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        metrics/mAP_0.5 ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   metrics/mAP_0.5:0.95 ▁▄█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/box_loss █▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/obj_loss █▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val/cls_loss █▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr0 ▁█▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr1 ▁█▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  x/lr2 █▅▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               _runtime ▁▄██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             _timestamp ▁▄██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  _step ▁▃▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /content/yolov5/wandb/offline-run-20210816_191508-1c9dnp0b\u001b[0m\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "id": "HnqTjJjEX9qI",
        "outputId": "676e86e8-5cef-4d42-bf0c-15612f706cea"
      },
      "source": [
        "# Tensorboard  (optional)\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs/train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mThis cell output is too large and can only be displayed while logged in.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "Nn2o1Og4drMY",
        "outputId": "62fa358f-bd79-46e0-97dc-9b1217e7b691"
      },
      "source": [
        "# Weights & Biases  (optional)\n",
        "%pip install -q wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.6 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDGNF_c1eUgm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6wmgasKh_ow"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}